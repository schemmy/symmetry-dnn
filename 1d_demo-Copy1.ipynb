{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. ]\n",
      " [ 1. ]\n",
      " [ 0.1]\n",
      " [-0.1]] [[1]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAD8CAYAAABkbJM/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVITA0OS1MTAkCQ1MTAkSU3+P7VUDC73mAluAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128ed3240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "random.seed(3)\n",
    "\n",
    "def rand_cluster(n,c,r):\n",
    "    \"\"\"returns n random points in disk of radius r centered at c\"\"\"\n",
    "    x = c\n",
    "    points = []\n",
    "    for i in range(n):\n",
    "        theta = 2*math.pi*random.random()\n",
    "        s = r*random.random()\n",
    "        points.append([x+s*math.cos(theta)])\n",
    "    return points\n",
    "\n",
    "def rand_clusters(k,n,r, a,b,c,d):\n",
    "    \"\"\"return k clusters of n points each in random disks of radius r\n",
    "    where the centers of the disk are chosen randomly in [a,b]x[c,d]\"\"\"\n",
    "    clusters = []\n",
    "    for _ in range(k):\n",
    "        x = a + (b-a)*random.random()\n",
    "        y = c + (d-c)*random.random()\n",
    "        clusters.extend(rand_cluster(n,x,r))\n",
    "    return clusters\n",
    "\n",
    "n = 2\n",
    "X = rand_clusters(2,n,0.8,-1,1,-1,1)\n",
    "data = np.array(X)\n",
    "label = np.transpose(np.array([[1]*n + [0]*n]))\n",
    "\n",
    "# label = np.array([1]*n + [0]*n)\n",
    "data = []\n",
    "label = []\n",
    "data.append([-1])\n",
    "data.append([1])\n",
    "data.append([0.1])\n",
    "data.append([-0.1])\n",
    "label.append([1])\n",
    "label.append([1])\n",
    "label.append([0])\n",
    "label.append([0])\n",
    "data = np.array(data)\n",
    "label = np.array(label)\n",
    "print (data, label)\n",
    "\n",
    "plt.scatter(data[:n], [0]*n, color=['red'])\n",
    "plt.scatter(data[n:], [0]*n, color=['green'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "\n",
    "# def weight_variable(shape, name):\n",
    "#     initial = tf.truncated_normal(shape=shape, stddev=0.1)\n",
    "#     return tf.get_variable(name=name, initializer=initial)\n",
    "\n",
    "# def bias_variable(shape, name):\n",
    "#     initial = tf.constant(0.1, shape=shape)\n",
    "#     return tf.get_variable(name=name, initializer=initial)\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 1])\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "\n",
    "n_input = 1\n",
    "n_hidden = 2\n",
    "n_output = 1\n",
    "lmd = 0\n",
    "\n",
    "parameters = tf.Variable(tf.concat([tf.truncated_normal([n_input * n_hidden]), tf.zeros([n_hidden]),\\\n",
    "                                    tf.truncated_normal([n_hidden * n_output]), tf.zeros([n_output]),\\\n",
    "                                   ], 0))\n",
    "\n",
    "idx_from = 0 \n",
    "weights1 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_input*n_hidden]), [n_input, n_hidden])\n",
    "idx_from = idx_from + n_input*n_hidden\n",
    "biases1 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_hidden]), [n_hidden])\n",
    "hidden = tf.nn.relu(tf.matmul(x, weights1) + biases1)\n",
    "\n",
    "idx_from = idx_from + n_hidden\n",
    "weights2 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_hidden*n_output]), [n_hidden, n_output])\n",
    "idx_from = idx_from + n_hidden*n_output\n",
    "biases2 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_output]), [n_output])\n",
    "y = tf.nn.relu(tf.matmul(hidden, weights2) + biases2)\n",
    "\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.pow(y_ - y, 2), reduction_indices=[1])) #I also tried simply tf.nn.l2_loss(y_ - y)\n",
    "\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "grad = tf.gradients(loss, parameters)\n",
    "hess = tf.hessians(loss, parameters)\n",
    "train_step = optimizer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_accuracy():\n",
    "    pred = sess.run(y, feed_dict={x: data, y_: label})    \n",
    "    match = [(pred[i] - 0.5) * (label[i] - 0.5) > 0  for i in range(n*2)]\n",
    "    acc = sum(match)*1./2/n\n",
    "    return acc[0]\n",
    "\n",
    "def get_norm_grad():\n",
    "    nng = 0.\n",
    "    for gv in grad:\n",
    "#         print(str(sess.run(gv[0], feed_dict={x: data, y_: label})) + \" - \" + gv[1].name)\n",
    "        g = sess.run(gv, feed_dict={x: data, y_: label})\n",
    "#         print (g)\n",
    "        nng += np.linalg.norm(g) ** 2\n",
    "    return np.sqrt(nng)\n",
    "    \n",
    "     \n",
    "def displayH(a):\n",
    "    a = np.array(a[0])\n",
    "#     print (\"Matrix[\"+(\"%d\" %a.shape[0])+\"][\"+(\"%d\" %a.shape[1])+\"]\")\n",
    "    rows = a.shape[0]\n",
    "    cols = a.shape[1]\n",
    "    for i in range(0, rows):\n",
    "        for j in range(0, cols):\n",
    "            print(\"%0.2g \" %a[i,j], end=\"\")\n",
    "        print ()\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.1685, high_eig 0, low_eig 0.\n",
      "1\n",
      "Epoch 100, accuracy 75.00%, loss 0.272734, nng 0.2352, nnw 1.841, high_eig 1.999, low_eig -0.01816.\n",
      "2\n",
      "Epoch 100, accuracy 75.00%, loss 0.142793, nng 0.5006, nnw 1.594, high_eig 2.765, low_eig -0.276.\n",
      "3\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.404, high_eig 0, low_eig 0.\n",
      "4\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.034, high_eig 0, low_eig 0.\n",
      "5\n",
      "Epoch 100, accuracy 50.00%, loss 0.278486, nng 0.7663, nnw 1.134, high_eig 2.276, low_eig -0.4625.\n",
      "6\n",
      "Epoch 100, accuracy 75.00%, loss 0.247128, nng 0.7348, nnw 1.472, high_eig 4.136, low_eig -0.421.\n",
      "7\n",
      "Epoch 100, accuracy 75.00%, loss 0.383415, nng 0.9507, nnw 1.376, high_eig 4.476, low_eig -0.4531.\n",
      "8\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.6419, high_eig 0, low_eig 0.\n",
      "9\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.3007, high_eig 0, low_eig 0.\n",
      "10\n",
      "Epoch 100, accuracy 50.00%, loss 0.337189, nng 0.7961, nnw 0.3766, high_eig 2.475, low_eig -0.444.\n",
      "11\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.4751, high_eig 0, low_eig 0.\n",
      "12\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.156, high_eig 0, low_eig 0.\n",
      "13\n",
      "Epoch 100, accuracy 75.00%, loss 0.325853, nng 0.6929, nnw 1.741, high_eig 3.942, low_eig -0.353.\n",
      "14\n",
      "Epoch 100, accuracy 50.00%, loss 0.371581, nng 0.9729, nnw 1.411, high_eig 2.666, low_eig -0.51.\n",
      "15\n",
      "Epoch 100, accuracy 100.00%, loss 0.033250, nng 0.3438, nnw 1.962, high_eig 3.756, low_eig -0.1522.\n",
      "16\n",
      "Epoch 100, accuracy 75.00%, loss 0.281946, nng 0.6842, nnw 1.025, high_eig 2.387, low_eig -0.3027.\n",
      "17\n",
      "Epoch 100, accuracy 75.00%, loss 0.303407, nng 0.4967, nnw 1.837, high_eig 5.29, low_eig -0.06116.\n",
      "18\n",
      "Epoch 100, accuracy 75.00%, loss 0.252607, nng 0.0436, nnw 1.143, high_eig 3.009, low_eig -0.02836.\n",
      "19\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 2.205, high_eig 0, low_eig 0.\n",
      "20\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.745, high_eig 0, low_eig 0.\n",
      "21\n",
      "Epoch 100, accuracy 75.00%, loss 0.303095, nng 0.3209, nnw 1.461, high_eig 1.734, low_eig -0.09339.\n",
      "22\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.249, high_eig 0, low_eig 0.\n",
      "23\n",
      "Epoch 100, accuracy 50.00%, loss 0.364009, nng 0.9185, nnw 0.772, high_eig 3.142, low_eig -0.4613.\n",
      "24\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.7162, high_eig 0, low_eig 0.\n",
      "25\n",
      "Epoch 100, accuracy 75.00%, loss 0.247463, nng 0.5344, nnw 0.5414, high_eig 4.096, low_eig -0.6738.\n",
      "26\n",
      "Epoch 100, accuracy 75.00%, loss 0.279852, nng 0.6579, nnw 0.9827, high_eig 7.322, low_eig -0.2443.\n",
      "27\n",
      "Epoch 100, accuracy 50.00%, loss 0.313734, nng 0.8282, nnw 1.24, high_eig 2.355, low_eig -0.5349.\n",
      "28\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.9267, high_eig 0, low_eig 0.\n",
      "29\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.027, high_eig 0, low_eig 0.\n",
      "30\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.315, high_eig 0, low_eig 0.\n",
      "31\n",
      "Epoch 100, accuracy 75.00%, loss 0.439618, nng 1.284, nnw 1.659, high_eig 4.771, low_eig -0.5027.\n",
      "32\n",
      "Epoch 100, accuracy 50.00%, loss 0.362836, nng 0.464, nnw 1.234, high_eig 1.377, low_eig -0.2449.\n",
      "33\n",
      "Epoch 100, accuracy 75.00%, loss 0.285191, nng 0.4611, nnw 1.785, high_eig 3.296, low_eig -0.2201.\n",
      "34\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.577, high_eig 0, low_eig 0.\n",
      "35\n",
      "Epoch 100, accuracy 50.00%, loss 0.320008, nng 0.406, nnw 1.651, high_eig 1.987, low_eig -0.03033.\n",
      "36\n",
      "Epoch 100, accuracy 75.00%, loss 0.235821, nng 0.4622, nnw 0.5034, high_eig 4.763, low_eig -0.06288.\n",
      "37\n",
      "Epoch 100, accuracy 75.00%, loss 0.281042, nng 0.6839, nnw 0.7854, high_eig 2.667, low_eig -0.2939.\n",
      "38\n",
      "Epoch 100, accuracy 50.00%, loss 0.349971, nng 0.9221, nnw 1.408, high_eig 2.417, low_eig -0.5499.\n",
      "39\n",
      "Epoch 100, accuracy 50.00%, loss 0.435053, nng 0.4611, nnw 0.582, high_eig 1.598, low_eig -0.4841.\n",
      "40\n",
      "Epoch 100, accuracy 50.00%, loss 0.424992, nng 0.4597, nnw 1.076, high_eig 1.111, low_eig -0.4775.\n",
      "41\n",
      "Epoch 100, accuracy 75.00%, loss 0.235879, nng 0.4655, nnw 0.9764, high_eig 3.327, low_eig -0.06671.\n",
      "42\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.2471, high_eig 0, low_eig 0.\n",
      "43\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.062, high_eig 0, low_eig 0.\n",
      "44\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.566, high_eig 0, low_eig 0.\n",
      "45\n",
      "Epoch 100, accuracy 100.00%, loss 0.092833, nng 0.4964, nnw 0.7509, high_eig 3.496, low_eig -0.09709.\n",
      "46\n",
      "Epoch 100, accuracy 75.00%, loss 0.238261, nng 0.5229, nnw 1.333, high_eig 5.058, low_eig -0.09895.\n",
      "47\n",
      "Epoch 100, accuracy 50.00%, loss 0.294922, nng 0.8204, nnw 1.379, high_eig 3.882, low_eig -0.3154.\n",
      "48\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.923, high_eig 0, low_eig 0.\n",
      "49\n",
      "Epoch 100, accuracy 75.00%, loss 0.264347, nng 0.6474, nnw 0.3654, high_eig 6.137, low_eig -0.2116.\n",
      "50\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.6203, high_eig 0, low_eig 0.\n",
      "51\n",
      "Epoch 100, accuracy 75.00%, loss 0.382659, nng 1.052, nnw 1.167, high_eig 4.509, low_eig -0.4112.\n",
      "52\n",
      "Epoch 100, accuracy 75.00%, loss 0.337266, nng 0.7345, nnw 1.587, high_eig 3.767, low_eig -0.3809.\n",
      "53\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.5126, high_eig 0, low_eig 0.\n",
      "54\n",
      "Epoch 100, accuracy 75.00%, loss 0.278772, nng 0.7878, nnw 1.135, high_eig 7.495, low_eig -0.2395.\n",
      "55\n",
      "Epoch 100, accuracy 75.00%, loss 0.247170, nng 0.5491, nnw 0.5991, high_eig 3.357, low_eig -0.151.\n",
      "56\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.934, high_eig 0, low_eig 0.\n",
      "57\n",
      "Epoch 100, accuracy 50.00%, loss 0.390227, nng 0.8071, nnw 0.2942, high_eig 2.042, low_eig -0.5595.\n",
      "58\n",
      "Epoch 100, accuracy 75.00%, loss 0.212046, nng 1.026, nnw 1.546, high_eig 8.215, low_eig -0.4322.\n",
      "59\n",
      "Epoch 100, accuracy 50.00%, loss 0.422732, nng 0.4554, nnw 1.281, high_eig 1.667, low_eig -0.4304.\n",
      "60\n",
      "Epoch 100, accuracy 50.00%, loss 0.295477, nng 0.8091, nnw 0.3865, high_eig 3.949, low_eig -0.355.\n",
      "61\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.959, high_eig 0, low_eig 0.\n",
      "62\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.152, high_eig 0, low_eig 0.\n",
      "63\n",
      "Epoch 100, accuracy 75.00%, loss 0.090867, nng 0.36, nnw 1.24, high_eig 3.329, low_eig -0.1616.\n",
      "64\n",
      "Epoch 100, accuracy 75.00%, loss 0.275880, nng 0.7083, nnw 1.166, high_eig 4.658, low_eig -0.2544.\n",
      "65\n",
      "Epoch 100, accuracy 75.00%, loss 0.295000, nng 0.5458, nnw 1.213, high_eig 3.974, low_eig -0.3161.\n",
      "66\n",
      "Epoch 100, accuracy 75.00%, loss 0.255400, nng 0.08085, nnw 1.415, high_eig 2.251, low_eig -0.0005827.\n",
      "67\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.3847, high_eig 0, low_eig 0.\n",
      "68\n",
      "Epoch 100, accuracy 75.00%, loss 0.261336, nng 0.2769, nnw 1.008, high_eig 3.58, low_eig -0.171.\n",
      "69\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.448, high_eig 0, low_eig 0.\n",
      "70\n",
      "Epoch 100, accuracy 50.00%, loss 0.340711, nng 0.4386, nnw 1.393, high_eig 1.491, low_eig -0.1749.\n",
      "71\n",
      "Epoch 100, accuracy 75.00%, loss 0.247497, nng 0.7061, nnw 1.23, high_eig 2.38, low_eig -0.4708.\n",
      "72\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.648, high_eig 0, low_eig 0.\n",
      "73\n",
      "Epoch 100, accuracy 50.00%, loss 0.324880, nng 0.7836, nnw 0.5392, high_eig 3.087, low_eig -0.3932.\n",
      "74\n",
      "Epoch 100, accuracy 75.00%, loss 0.372212, nng 0.9374, nnw 1.253, high_eig 4.868, low_eig -0.4107.\n",
      "75\n",
      "Epoch 100, accuracy 50.00%, loss 0.366285, nng 0.7918, nnw 0.4733, high_eig 2.075, low_eig -0.3723.\n",
      "76\n",
      "Epoch 100, accuracy 75.00%, loss 0.259138, nng 0.3167, nnw 1.375, high_eig 5.795, low_eig -0.135.\n",
      "77\n",
      "Epoch 100, accuracy 50.00%, loss 0.393773, nng 0.8404, nnw 0.6557, high_eig 2.073, low_eig -0.5668.\n",
      "78\n",
      "Epoch 100, accuracy 75.00%, loss 0.240839, nng 0.5086, nnw 0.7799, high_eig 3.2, low_eig -0.111.\n",
      "79\n",
      "Epoch 100, accuracy 50.00%, loss 0.474130, nng 0.4479, nnw 0.7614, high_eig 1.568, low_eig -0.6263.\n",
      "80\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.6248, high_eig 0, low_eig 0.\n",
      "81\n",
      "Epoch 100, accuracy 75.00%, loss 0.266446, nng 0.1922, nnw 1.478, high_eig 2.328, low_eig -0.003268.\n",
      "82\n",
      "Epoch 100, accuracy 50.00%, loss 0.266700, nng 0.7421, nnw 0.8459, high_eig 2.647, low_eig -0.4897.\n",
      "83\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.6688, high_eig 0, low_eig 0.\n",
      "84\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.069, high_eig 0, low_eig 0.\n",
      "85\n",
      "Epoch 100, accuracy 75.00%, loss 0.254963, nng 0.06697, nnw 1.715, high_eig 2.693, low_eig -0.0004175.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.604, high_eig 0, low_eig 0.\n",
      "87\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.212, high_eig 0, low_eig 0.\n",
      "88\n",
      "Epoch 100, accuracy 75.00%, loss 0.272626, nng 0.2651, nnw 2.217, high_eig 2.302, low_eig -0.01386.\n",
      "89\n",
      "Epoch 100, accuracy 75.00%, loss 0.293580, nng 0.2958, nnw 1.052, high_eig 1.903, low_eig -0.07098.\n",
      "90\n",
      "Epoch 100, accuracy 75.00%, loss 0.271034, nng 0.2025, nnw 1.776, high_eig 1.954, low_eig -0.02178.\n",
      "91\n",
      "Epoch 100, accuracy 50.00%, loss 0.348077, nng 0.8726, nnw 0.8063, high_eig 2.772, low_eig -0.4565.\n",
      "92\n",
      "Epoch 100, accuracy 50.00%, loss 0.335215, nng 0.4072, nnw 1.024, high_eig 1.883, low_eig -0.1235.\n",
      "93\n",
      "Epoch 100, accuracy 75.00%, loss 0.262775, nng 0.6642, nnw 1.208, high_eig 4.029, low_eig -0.2155.\n",
      "94\n",
      "Epoch 100, accuracy 75.00%, loss 0.447905, nng 1.22, nnw 1.419, high_eig 5.217, low_eig -0.6208.\n",
      "95\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.83, high_eig 0, low_eig 0.\n",
      "96\n",
      "Epoch 100, accuracy 50.00%, loss 0.425967, nng 0.6751, nnw 1.995, high_eig 2.406, low_eig -0.3451.\n",
      "97\n",
      "Epoch 100, accuracy 50.00%, loss 0.452424, nng 0.5274, nnw 0.8039, high_eig 1.659, low_eig -0.501.\n",
      "98\n",
      "Epoch 100, accuracy 50.00%, loss 0.414982, nng 0.4586, nnw 1.261, high_eig 1.202, low_eig -0.4434.\n",
      "99\n",
      "Epoch 100, accuracy 75.00%, loss 0.237823, nng 0.5056, nnw 1.888, high_eig 4.675, low_eig -0.08568.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "l = 1e-3\n",
    "dic = {}\n",
    "\n",
    "for r in range(100):\n",
    "    \n",
    "    flag = 1\n",
    "    tf.global_variables_initializer().run()\n",
    "    w = sess.run([parameters], feed_dict={x: data, y_: label, lr: l})\n",
    "    dic[r] = w\n",
    " \n",
    "    for i in range(100):\n",
    "        sess.run(train_step, feed_dict={x: data, y_: label, lr: l})\n",
    "        nng = get_norm_grad()\n",
    "        if nng < 1e-4:\n",
    "            flag = 1\n",
    "            break\n",
    "\n",
    "    if flag == 1:\n",
    "        v, H, w = sess.run([loss, hess, parameters], feed_dict={x: data, y_: label, lr: l})    \n",
    "        nng = get_norm_grad()\n",
    "        eigs = sorted(np.linalg.eigvals(H)[0])\n",
    "        print (r)\n",
    "        print(\"Epoch {}, accuracy {:.2f}%, loss {:.6f}, nng {:.4g}, nnw {:.4g}, high_eig {:.4g}, low_eig {:.4g}.\"\\\n",
    "                .format(i+1, get_accuracy()*100, v, nng, np.linalg.norm(w[:n_hidden]), max(eigs), min(eigs) ))\n",
    "        \n",
    "#         print (w)\n",
    "#             h = sess.run([y], feed_dict={x: data, y_: label})    \n",
    "#         print (h)\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3.19:\n",
    "\n",
    "Relu is helpful to obtain flat region. For example, zero is not a first order stationary point while using sigmoid, but indeed is while using relu.\n",
    "    \n",
    "For sigmoid, point far away from 0 tends to have tiny gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sol = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.54967874  0.46465215  0.          0.          0.85924828  1.13367045\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "idx = 45\n",
    "print (dic[idx][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, accuracy 100.00%, loss 0.000000, nng 8.756e-08, nnw 1.312, high_eig 3.455, low_eig -3.519e-08.\n",
      "[-0.9774105   0.87449503 -0.09644938 -0.08832376  1.13679051  1.27385521\n",
      " -0.00146833]\n"
     ]
    }
   ],
   "source": [
    "private_init = parameters.assign(dic[idx][0])\n",
    "sess = tf.InteractiveSession()\n",
    "l = 5e-1\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "sess.run(private_init)    \n",
    "v, H, w = sess.run([loss, hess, parameters], feed_dict={x: data, y_: label, lr: l})    \n",
    "nng = get_norm_grad()\n",
    "# print (sess.run(grad[0], feed_dict={x: data, y_: label}))\n",
    "\n",
    "for i in range(100):\n",
    "    sess.run(train_step, feed_dict={x: data, y_: label, lr: l})\n",
    "    v, H, w = sess.run([loss, hess, parameters], feed_dict={x: data, y_: label, lr: l})    \n",
    "    nng = get_norm_grad()\n",
    "    eigs = sorted(np.linalg.eigvals(H)[0])\n",
    "print(\"Epoch {}, accuracy {:.2f}%, loss {:.6f}, nng {:.4g}, nnw {:.4g}, high_eig {:.4g}, low_eig {:.4g}.\"\\\n",
    "            .format(i+1, get_accuracy()*100, v, nng, np.linalg.norm(w[:n_hidden]), max(eigs), min(eigs) ))\n",
    "#     print (sess.run(grad[0], feed_dict={x: data, y_: label}))\n",
    "#     print (w)\n",
    "sol[idx] = w\n",
    "print (sol[idx])\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{15: array([ 1.75133431, -1.00075877, -0.07483343, -0.01163835,  0.65504843,\n",
       "         1.11026835, -0.09818926], dtype=float32),\n",
       " 45: array([-0.9774105 ,  0.87449503, -0.09644938, -0.08832376,  1.13679051,\n",
       "         1.27385521, -0.00146833], dtype=float32)}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.79397285 -0.81013817  0.          0.          0.74770951  0.9456349   0.        ]\n",
      "Epoch 100, accuracy 100.00%, loss 7.56339e-15, nng 7.985e-08, nnw 2.017, high_eig 3.446, low_eig -9.984e-09.\n",
      "[ 1.75133431 -1.00075877 -0.07483343 -0.01163835  0.65504843  1.11026835\n",
      " -0.09818926]\n",
      "Epoch 100, accuracy 100.00%, loss 7.56339e-15, nng 0.9949, nnw 2.928, high_eig 3.446, low_eig -9.984e-09.\n"
     ]
    }
   ],
   "source": [
    "w0 = dic[15][0]\n",
    "print (w0)\n",
    "private_init = parameters.assign(w0)\n",
    "sess = tf.InteractiveSession()\n",
    "l = 5e-1\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "sess.run(private_init)    \n",
    "v, H, w = sess.run([loss, hess, parameters], feed_dict={x: data, y_: label, lr: l})    \n",
    "nng = get_norm_grad()\n",
    "# print (sess.run(grad[0], feed_dict={x: data, y_: label}))\n",
    "\n",
    "for i in range(100):\n",
    "    sess.run(train_step, feed_dict={x: data, y_: label, lr: l})\n",
    "    v, H, w = sess.run([loss, hess, parameters], feed_dict={x: data, y_: label, lr: l})    \n",
    "    nng = get_norm_grad()\n",
    "    eigs = sorted(np.linalg.eigvals(H)[0])\n",
    "print(\"Epoch {}, accuracy {:.2f}%, loss {:.6g}, nng {:.4g}, nnw {:.4g}, high_eig {:.4g}, low_eig {:.4g}.\"\\\n",
    "            .format(i+1, get_accuracy()*100, v, nng, np.linalg.norm(w[:n_hidden]), max(eigs), min(eigs) ))\n",
    "print (w)\n",
    "w[0] += 1e-0\n",
    "private_init = parameters.assign(w)\n",
    "sess.run(private_init)    \n",
    "nng = get_norm_grad()\n",
    "print(\"Epoch {}, accuracy {:.2f}%, loss {:.6g}, nng {:.4g}, nnw {:.4g}, high_eig {:.4g}, low_eig {:.4g}.\"\\\n",
    "            .format(i+1, get_accuracy()*100, v, nng, np.linalg.norm(w[:n_hidden]), max(eigs), min(eigs) ))\n",
    "\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.20:\n",
    "    This example shows, initialization is more important than optimization, to reach 100% training accuracy. \n",
    "    \n",
    "    Also, for 1d data, weights are not important. It is bias that works. It may be possible to visualize the landscape since there are only 3 bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
