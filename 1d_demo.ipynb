{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAD8CAYAAAC2PJlnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11fac4e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "random.seed(3)\n",
    "\n",
    "def rand_cluster(n,c,r):\n",
    "    \"\"\"returns n random points in disk of radius r centered at c\"\"\"\n",
    "    x = c\n",
    "    points = []\n",
    "    for i in range(n):\n",
    "        theta = 2*math.pi*random.random()\n",
    "        s = r*random.random()\n",
    "        points.append([x+s*math.cos(theta)])\n",
    "    return points\n",
    "\n",
    "def rand_clusters(k,n,r, a,b,c,d):\n",
    "    \"\"\"return k clusters of n points each in random disks of radius r\n",
    "    where the centers of the disk are chosen randomly in [a,b]x[c,d]\"\"\"\n",
    "    clusters = []\n",
    "    for _ in range(k):\n",
    "        x = a + (b-a)*random.random()\n",
    "        y = c + (d-c)*random.random()\n",
    "        clusters.extend(rand_cluster(n,x,r))\n",
    "    return clusters\n",
    "\n",
    "n = 50\n",
    "X = rand_clusters(2,50,0.8,-1,1,-1,1)\n",
    "data = np.array(X)\n",
    "label = np.transpose(np.array([[1]*n + [0]*n]))\n",
    "# label = np.array([1]*n + [0]*n)\n",
    "# print (data, label)\n",
    "\n",
    "plt.scatter(data[:n], [0]*n, color=['red'])\n",
    "plt.scatter(data[n:], [0]*n, color=['green'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "\n",
    "# def weight_variable(shape, name):\n",
    "#     initial = tf.truncated_normal(shape=shape, stddev=0.1)\n",
    "#     return tf.get_variable(name=name, initializer=initial)\n",
    "\n",
    "# def bias_variable(shape, name):\n",
    "#     initial = tf.constant(0.1, shape=shape)\n",
    "#     return tf.get_variable(name=name, initializer=initial)\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 1])\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "\n",
    "n_input = 1\n",
    "n_hidden = 2\n",
    "n_output = 1\n",
    "lmd = 0\n",
    "\n",
    "parameters = tf.Variable(tf.concat([tf.truncated_normal([n_input * n_hidden]), tf.zeros([n_hidden]),\\\n",
    "                                    tf.truncated_normal([n_hidden * n_output]), tf.zeros([n_output]),\\\n",
    "                                   ], 0))\n",
    "\n",
    "idx_from = 0 \n",
    "weights1 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_input*n_hidden]), [n_input, n_hidden])\n",
    "idx_from = idx_from + n_input*n_hidden\n",
    "biases1 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_hidden]), [n_hidden])\n",
    "hidden = tf.nn.relu(tf.matmul(x, weights1) + biases1)\n",
    "\n",
    "idx_from = idx_from + n_hidden\n",
    "weights2 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_hidden*n_output]), [n_hidden, n_output])\n",
    "idx_from = idx_from + n_hidden*n_output\n",
    "biases2 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_output]), [n_output])\n",
    "y = tf.nn.relu(tf.matmul(hidden, weights2) + biases2)\n",
    "\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.pow(y_ - y, 2), reduction_indices=[1])) #I also tried simply tf.nn.l2_loss(y_ - y)\n",
    "\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "grad = tf.gradients(loss, parameters)\n",
    "hess = tf.hessians(loss, parameters)\n",
    "train_step = optimizer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_accuracy():\n",
    "    pred = sess.run(y, feed_dict={x: data, y_: label})    \n",
    "    match = [(pred[i] - 0.5) * (label[i] - 0.5) > 0  for i in range(n*2)]\n",
    "    acc = sum(match)*1./2/n\n",
    "    return acc[0]\n",
    "\n",
    "def get_norm_grad():\n",
    "    nng = 0.\n",
    "    for gv in grad:\n",
    "#         print(str(sess.run(gv[0], feed_dict={x: data, y_: label})) + \" - \" + gv[1].name)\n",
    "        g = sess.run(gv, feed_dict={x: data, y_: label})\n",
    "#         print (g)\n",
    "        nng += np.linalg.norm(g) ** 2\n",
    "    return np.sqrt(nng)\n",
    "    \n",
    "     \n",
    "def displayH(a):\n",
    "    a = np.array(a[0])\n",
    "#     print (\"Matrix[\"+(\"%d\" %a.shape[0])+\"][\"+(\"%d\" %a.shape[1])+\"]\")\n",
    "    rows = a.shape[0]\n",
    "    cols = a.shape[1]\n",
    "    for i in range(0, rows):\n",
    "        for j in range(0, cols):\n",
    "            print(\"%0.2g \" %a[i,j], end=\"\")\n",
    "        print ()\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.55369854 -0.60906827  4.5541687   5.00958586  9.1083374   2.11097527\n",
      "  4.5541687 ]\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.473, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.13842463  0.45226708  0.86145782 -0.75239646 -1.27708435  0.57225621\n",
      " -0.83854216]\n",
      "Epoch 2, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.473, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.13842463  0.45226708  0.86145782 -0.75239646 -1.27708435  0.57225621\n",
      " -0.83854216]\n",
      "Epoch 3, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.473, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.13842463  0.45226708  0.86145782 -0.75239646 -1.27708435  0.57225621\n",
      " -0.83854216]\n",
      "Epoch 4, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.473, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.13842463  0.45226708  0.86145782 -0.75239646 -1.27708435  0.57225621\n",
      " -0.83854216]\n",
      "Epoch 5, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.473, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.13842463  0.45226708  0.86145782 -0.75239646 -1.27708435  0.57225621\n",
      " -0.83854216]\n",
      "Epoch 6, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.473, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.13842463  0.45226708  0.86145782 -0.75239646 -1.27708435  0.57225621\n",
      " -0.83854216]\n",
      "Epoch 7, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.473, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.13842463  0.45226708  0.86145782 -0.75239646 -1.27708435  0.57225621\n",
      " -0.83854216]\n",
      "Epoch 8, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.473, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.13842463  0.45226708  0.86145782 -0.75239646 -1.27708435  0.57225621\n",
      " -0.83854216]\n",
      "Epoch 9, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.473, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.13842463  0.45226708  0.86145782 -0.75239646 -1.27708435  0.57225621\n",
      " -0.83854216]\n",
      "Epoch 10, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.473, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.13842463  0.45226708  0.86145782 -0.75239646 -1.27708435  0.57225621\n",
      " -0.83854216]\n"
     ]
    }
   ],
   "source": [
    "# 太巧合了！ 绝对不能改！\n",
    "w0 = np.array([0,0.3,\n",
    "              2,0.5,\n",
    "              1,1.1,\n",
    "              0.3])\n",
    "private_init = parameters.assign(w0)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "l = 0.25 # learning rate 可以随便改\n",
    "dic = {}\n",
    "for _ in range(1):\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    sess.run(private_init)    \n",
    "    v, H, w = sess.run([loss, hess, parameters], feed_dict={x: data, y_: label, lr: l})    \n",
    "    nng = get_norm_grad()\n",
    "    print (sess.run(grad[0], feed_dict={x: data, y_: label}))\n",
    "#     print (w)\n",
    "#     h = sess.run([y], feed_dict={x: data, y_: label})    \n",
    "#     displayH(h)\n",
    "    \n",
    "    for i in range(10):\n",
    "        sess.run(train_step, feed_dict={x: data, y_: label, lr: l})\n",
    "        \n",
    "#         if i % 1 == 0:\n",
    "        v, H, w = sess.run([loss, hess, parameters], feed_dict={x: data, y_: label, lr: l})    \n",
    "        nng = get_norm_grad()\n",
    "        eigs = sorted(np.linalg.eigvals(H)[0])\n",
    "        print(\"Epoch {}, accuracy {:.2f}%, loss {:.6f}, nng {:.4g}, nnw {:.4g}, high_eig {:.4g}, low_eig {:.4g}.\"\\\n",
    "                .format(i+1, get_accuracy()*100, v, nng, np.linalg.norm(w[:n_hidden]), max(eigs), min(eigs) ))\n",
    "        print (sess.run(grad[0], feed_dict={x: data, y_: label}))\n",
    "        print (w)\n",
    "#         h = sess.run([y], feed_dict={x: data, y_: label})    \n",
    "#         displayH(h)\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.245, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108627, nng 0.0006215, nnw 0.9314, high_eig 4.004, low_eig -0.0007287.\n",
      "Epoch 591, accuracy 86.00%, loss 0.118555, nng 9.736e-05, nnw 0.7415, high_eig 7.86, low_eig -4.711e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.682, high_eig 0, low_eig 0.\n",
      "Epoch 176, accuracy 84.00%, loss 0.108471, nng 9.67e-05, nnw 1.048, high_eig 3.629, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.494, high_eig 0, low_eig 0.\n",
      "Epoch 357, accuracy 50.00%, loss 0.500000, nng 9.996e-05, nnw 1.647, high_eig 0.05061, low_eig -6.79e-05.\n",
      "Epoch 499, accuracy 86.00%, loss 0.118556, nng 9.985e-05, nnw 1.018, high_eig 4.636, low_eig -3.178e-05.\n",
      "Epoch 330, accuracy 86.00%, loss 0.118555, nng 9.867e-05, nnw 1.056, high_eig 4.184, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.176, high_eig 0, low_eig 0.\n",
      "Epoch 318, accuracy 50.00%, loss 0.500000, nng 9.899e-05, nnw 1.206, high_eig 0.0662, low_eig -5.629e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.7825, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108495, nng 0.0002646, nnw 1.299, high_eig 3.914, low_eig -5.757e-05.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.117230, nng 0.001234, nnw 1.188, high_eig 4.259, low_eig -0.0007585.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.7332, high_eig 0, low_eig 0.\n",
      "Epoch 321, accuracy 86.00%, loss 0.118555, nng 9.875e-05, nnw 1.043, high_eig 4.201, low_eig 0.\n",
      "Epoch 443, accuracy 50.00%, loss 0.500000, nng 9.939e-05, nnw 0.961, high_eig 0.03632, low_eig -9.786e-05.\n",
      "Epoch 219, accuracy 50.00%, loss 0.500000, nng 9.815e-05, nnw 0.8425, high_eig 0.09542, low_eig -5.964e-05.\n",
      "Epoch 460, accuracy 84.00%, loss 0.109045, nng 5.646e-05, nnw 1.665, high_eig 3.094, low_eig -5.133e-06.\n",
      "Epoch 897, accuracy 86.00%, loss 0.118557, nng 9.999e-05, nnw 1.55, high_eig 6.68, low_eig -6.425e-05.\n",
      "Epoch 938, accuracy 86.00%, loss 0.118556, nng 9.984e-05, nnw 1.833, high_eig 5.204, low_eig -0.000151.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.577, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108371, nng 0.0002238, nnw 1.477, high_eig 3.438, low_eig -1.175e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 2.012, high_eig 0, low_eig 0.\n",
      "Epoch 106, accuracy 84.00%, loss 0.109045, nng 9.474e-05, nnw 0.9194, high_eig 5.044, low_eig 0.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.116655, nng 0.01615, nnw 1.215, high_eig 3.448, low_eig -0.0001939.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.146, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108604, nng 0.001621, nnw 1.729, high_eig 5.11, low_eig -4.087e-06.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.6406, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.6971, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108604, nng 0.000481, nnw 1.327, high_eig 5.712, low_eig -8.605e-06.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.3544, high_eig 0, low_eig 0.\n",
      "Epoch 199, accuracy 50.00%, loss 0.500000, nng 9.945e-05, nnw 0.6046, high_eig 0.08952, low_eig -6.239e-05.\n",
      "Epoch 163, accuracy 84.00%, loss 0.109045, nng 9.901e-05, nnw 1.399, high_eig 3.147, low_eig -0.001657.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.614, high_eig 0, low_eig 0.\n",
      "Epoch 398, accuracy 86.00%, loss 0.118555, nng 9.99e-05, nnw 1.248, high_eig 4.647, low_eig -4.443e-05.\n",
      "Epoch 235, accuracy 86.00%, loss 0.118555, nng 9.954e-05, nnw 0.7645, high_eig 4.963, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.7644, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.8802, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.405, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.2983, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.983, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108604, nng 0.001724, nnw 1.609, high_eig 5.096, low_eig -1.38e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.566, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 2.634, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.531, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.5232, high_eig 0, low_eig 0.\n",
      "Epoch 341, accuracy 86.00%, loss 0.118555, nng 9.838e-05, nnw 0.8216, high_eig 4.971, low_eig 0.\n",
      "Epoch 242, accuracy 50.00%, loss 0.500000, nng 9.892e-05, nnw 1.054, high_eig 0.09247, low_eig -3.652e-05.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.109086, nng 0.003023, nnw 1.02, high_eig 4.248, low_eig -0.00267.\n",
      "Epoch 216, accuracy 84.00%, loss 0.109045, nng 9.882e-05, nnw 1.657, high_eig 2.965, low_eig -9.295e-06.\n",
      "Epoch 77, accuracy 50.00%, loss 0.250000, nng 1.46e-05, nnw 0.4311, high_eig 2, low_eig 0.\n",
      "Epoch 307, accuracy 50.00%, loss 0.250000, nng 1.736e-06, nnw 0.5071, high_eig 2, low_eig 0.\n",
      "Epoch 788, accuracy 86.00%, loss 0.118556, nng 9.994e-05, nnw 1.346, high_eig 4.49, low_eig -3.97e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.4908, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.116962, nng 0.0009874, nnw 1.981, high_eig 4.448, low_eig -3.775e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.431, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108604, nng 0.0005868, nnw 1.356, high_eig 6.052, low_eig -8.315e-05.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108604, nng 0.001493, nnw 1.315, high_eig 5.593, low_eig -0.0004812.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.057, high_eig 0, low_eig 0.\n",
      "Epoch 50, accuracy 50.00%, loss 0.250000, nng 5.774e-05, nnw 0.1603, high_eig 2, low_eig 0.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.117910, nng 0.004149, nnw 1.59, high_eig 7.874, low_eig -0.007105.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.975, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.002, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108604, nng 0.001532, nnw 1.962, high_eig 5.696, low_eig -0.0002906.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.117053, nng 0.0006035, nnw 1.328, high_eig 6.845, low_eig -2.657e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.5353, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.372, high_eig 0, low_eig 0.\n",
      "Epoch 546, accuracy 86.00%, loss 0.118556, nng 9.99e-05, nnw 1.55, high_eig 4.646, low_eig -4.68e-05.\n",
      "Epoch 392, accuracy 50.00%, loss 0.500000, nng 9.936e-05, nnw 1.86, high_eig 0.04642, low_eig -8.263e-05.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.116630, nng 0.007513, nnw 2.07, high_eig 2.992, low_eig -0.002253.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108607, nng 0.001598, nnw 1.496, high_eig 5.164, low_eig -3.912e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.668, high_eig 0, low_eig 0.\n",
      "Epoch 181, accuracy 50.00%, loss 0.500000, nng 9.852e-05, nnw 0.3681, high_eig 0.1059, low_eig -2.677e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.7279, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108606, nng 0.001531, nnw 1.143, high_eig 6.457, low_eig -0.0002586.\n",
      "Epoch 299, accuracy 86.00%, loss 0.118555, nng 9.897e-05, nnw 0.9157, high_eig 4.438, low_eig -9.799e-06.\n",
      "Epoch 155, accuracy 84.00%, loss 0.108471, nng 9.757e-05, nnw 1.105, high_eig 3.656, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108604, nng 0.001174, nnw 1.49, high_eig 5.169, low_eig -0.0003463.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.579, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.116879, nng 0.001837, nnw 1.701, high_eig 5.24, low_eig -0.0001702.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.469, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.6889, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108626, nng 0.0006158, nnw 1.736, high_eig 3.263, low_eig -0.0008155.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.8179, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.7119, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.116623, nng 0.004053, nnw 2.467, high_eig 3.678, low_eig -0.0003399.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, accuracy 84.00%, loss 0.108607, nng 0.007093, nnw 1.401, high_eig 7.929, low_eig -0.002277.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.8421, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.461, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.705, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.224, high_eig 0, low_eig 0.\n",
      "Epoch 371, accuracy 50.00%, loss 0.500000, nng 9.98e-05, nnw 1.65, high_eig 0.04222, low_eig -7.72e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.7313, high_eig 0, low_eig 0.\n",
      "Epoch 279, accuracy 86.00%, loss 0.118555, nng 9.989e-05, nnw 0.871, high_eig 4.508, low_eig 0.\n",
      "Epoch 173, accuracy 50.00%, loss 0.500000, nng 9.76e-05, nnw 0.3043, high_eig 0.1062, low_eig -2.599e-05.\n",
      "Epoch 698, accuracy 86.00%, loss 0.118555, nng 9.959e-05, nnw 1.683, high_eig 4.851, low_eig -0.0001166.\n",
      "Epoch 493, accuracy 86.00%, loss 0.118556, nng 9.964e-05, nnw 1.438, high_eig 6.129, low_eig -0.0001132.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.461, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108605, nng 0.002612, nnw 0.6741, high_eig 6.777, low_eig -0.0004539.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "l = 0.25\n",
    "dic = {}\n",
    "\n",
    "for _ in range(100):\n",
    "    \n",
    "    flag = 1\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(1000):\n",
    "        sess.run(train_step, feed_dict={x: data, y_: label, lr: l})\n",
    "        nng = get_norm_grad()\n",
    "        if nng < 1e-4:\n",
    "            flag = 1\n",
    "            break\n",
    "\n",
    "    if flag == 1:\n",
    "        v, H, w = sess.run([loss, hess, parameters], feed_dict={x: data, y_: label, lr: l})    \n",
    "        nng = get_norm_grad()\n",
    "        eigs = sorted(np.linalg.eigvals(H)[0])\n",
    "        print(\"Epoch {}, accuracy {:.2f}%, loss {:.6f}, nng {:.4g}, nnw {:.4g}, high_eig {:.4g}, low_eig {:.4g}.\"\\\n",
    "                .format(i+1, get_accuracy()*100, v, nng, np.linalg.norm(w[:n_hidden]), max(eigs), min(eigs) ))\n",
    "#             h = sess.run([y], feed_dict={x: data, y_: label})    \n",
    "#             print (h)\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3.19:\n",
    "\n",
    "Relu is helpful to obtain flat region. For example, zero is not a first order stationary point while using sigmoid, but indeed is while using relu.\n",
    "    \n",
    "For sigmoid, point far away from 0 tends to have tiny gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
