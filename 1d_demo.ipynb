{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAD8CAYAAAC2PJlnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1291f0438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "random.seed(3)\n",
    "\n",
    "def rand_cluster(n,c,r):\n",
    "    \"\"\"returns n random points in disk of radius r centered at c\"\"\"\n",
    "    x = c\n",
    "    points = []\n",
    "    for i in range(n):\n",
    "        theta = 2*math.pi*random.random()\n",
    "        s = r*random.random()\n",
    "        points.append([x+s*math.cos(theta)])\n",
    "    return points\n",
    "\n",
    "def rand_clusters(k,n,r, a,b,c,d):\n",
    "    \"\"\"return k clusters of n points each in random disks of radius r\n",
    "    where the centers of the disk are chosen randomly in [a,b]x[c,d]\"\"\"\n",
    "    clusters = []\n",
    "    for _ in range(k):\n",
    "        x = a + (b-a)*random.random()\n",
    "        y = c + (d-c)*random.random()\n",
    "        clusters.extend(rand_cluster(n,x,r))\n",
    "    return clusters\n",
    "\n",
    "n = 50\n",
    "X = rand_clusters(2,50,0.8,-1,1,-1,1)\n",
    "data = np.array(X)\n",
    "label = np.transpose(np.array([[1]*n + [0]*n]))\n",
    "# label = np.array([1]*n + [0]*n)\n",
    "# print (data, label)\n",
    "\n",
    "plt.scatter(data[:n], [0]*n, color=['red'])\n",
    "plt.scatter(data[n:], [0]*n, color=['green'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "\n",
    "# def weight_variable(shape, name):\n",
    "#     initial = tf.truncated_normal(shape=shape, stddev=0.1)\n",
    "#     return tf.get_variable(name=name, initializer=initial)\n",
    "\n",
    "# def bias_variable(shape, name):\n",
    "#     initial = tf.constant(0.1, shape=shape)\n",
    "#     return tf.get_variable(name=name, initializer=initial)\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 1])\n",
    "y_ = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "\n",
    "n_input = 1\n",
    "n_hidden = 2\n",
    "n_output = 1\n",
    "lmd = 0\n",
    "\n",
    "parameters = tf.Variable(tf.concat([tf.truncated_normal([n_input * n_hidden]), tf.zeros([n_hidden]),\\\n",
    "                                    tf.truncated_normal([n_hidden * n_output]), tf.zeros([n_output]),\\\n",
    "                                   ], 0))\n",
    "\n",
    "idx_from = 0 \n",
    "weights1 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_input*n_hidden]), [n_input, n_hidden])\n",
    "idx_from = idx_from + n_input*n_hidden\n",
    "biases1 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_hidden]), [n_hidden])\n",
    "hidden = tf.nn.relu(tf.matmul(x, weights1) + biases1)\n",
    "\n",
    "idx_from = idx_from + n_hidden\n",
    "weights2 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_hidden*n_output]), [n_hidden, n_output])\n",
    "idx_from = idx_from + n_hidden*n_output\n",
    "biases2 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_output]), [n_output])\n",
    "y = tf.nn.relu(tf.matmul(hidden, weights2) + biases2)\n",
    "\n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.pow(y_ - y, 2), reduction_indices=[1])) #I also tried simply tf.nn.l2_loss(y_ - y)\n",
    "\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "grad = tf.gradients(loss, parameters)\n",
    "hess = tf.hessians(loss, parameters)\n",
    "train_step = optimizer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_accuracy():\n",
    "    pred = sess.run(y, feed_dict={x: data, y_: label})    \n",
    "    match = [(pred[i] - 0.5) * (label[i] - 0.5) > 0  for i in range(n*2)]\n",
    "    acc = sum(match)*1./2/n\n",
    "    return acc[0]\n",
    "\n",
    "def get_norm_grad():\n",
    "    nng = 0.\n",
    "    for gv in grad:\n",
    "#         print(str(sess.run(gv[0], feed_dict={x: data, y_: label})) + \" - \" + gv[1].name)\n",
    "        g = sess.run(gv, feed_dict={x: data, y_: label})\n",
    "#         print (g)\n",
    "        nng += np.linalg.norm(g) ** 2\n",
    "    return np.sqrt(nng)\n",
    "    \n",
    "     \n",
    "def displayH(a):\n",
    "    a = np.array(a[0])\n",
    "#     print (\"Matrix[\"+(\"%d\" %a.shape[0])+\"][\"+(\"%d\" %a.shape[1])+\"]\")\n",
    "    rows = a.shape[0]\n",
    "    cols = a.shape[1]\n",
    "    for i in range(0, rows):\n",
    "        for j in range(0, cols):\n",
    "            print(\"%0.2g \" %a[i,j], end=\"\")\n",
    "        print ()\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -0.85587358  -0.85587358   5.86742735   5.86742735  11.7348547\n",
      "   2.67695093   5.86742592]\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.5567, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.2139684   0.51396841  0.53314316 -0.96685684 -1.93371367  0.33076227\n",
      " -0.46685648]\n",
      "Epoch 2, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.5567, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.2139684   0.51396841  0.53314316 -0.96685684 -1.93371367  0.33076227\n",
      " -0.46685648]\n",
      "Epoch 3, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.5567, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.2139684   0.51396841  0.53314316 -0.96685684 -1.93371367  0.33076227\n",
      " -0.46685648]\n",
      "Epoch 4, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.5567, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.2139684   0.51396841  0.53314316 -0.96685684 -1.93371367  0.33076227\n",
      " -0.46685648]\n",
      "Epoch 5, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.5567, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.2139684   0.51396841  0.53314316 -0.96685684 -1.93371367  0.33076227\n",
      " -0.46685648]\n",
      "Epoch 6, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.5567, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.2139684   0.51396841  0.53314316 -0.96685684 -1.93371367  0.33076227\n",
      " -0.46685648]\n",
      "Epoch 7, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.5567, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.2139684   0.51396841  0.53314316 -0.96685684 -1.93371367  0.33076227\n",
      " -0.46685648]\n",
      "Epoch 8, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.5567, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.2139684   0.51396841  0.53314316 -0.96685684 -1.93371367  0.33076227\n",
      " -0.46685648]\n",
      "Epoch 9, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.5567, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.2139684   0.51396841  0.53314316 -0.96685684 -1.93371367  0.33076227\n",
      " -0.46685648]\n",
      "Epoch 10, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.5567, high_eig 0, low_eig 0.\n",
      "[ 0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.2139684   0.51396841  0.53314316 -0.96685684 -1.93371367  0.33076227\n",
      " -0.46685648]\n"
     ]
    }
   ],
   "source": [
    "# 太巧合了！ 绝对不能改！\n",
    "w0 = np.array([0,0.3,\n",
    "              2,0.5,\n",
    "              1,1,\n",
    "              1])\n",
    "private_init = parameters.assign(w0)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "l = 0.25 # learning rate 可以随便改\n",
    "dic = {}\n",
    "for _ in range(1):\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    sess.run(private_init)    \n",
    "    v, H, w = sess.run([loss, hess, parameters], feed_dict={x: data, y_: label, lr: l})    \n",
    "    nng = get_norm_grad()\n",
    "    print (sess.run(grad[0], feed_dict={x: data, y_: label}))\n",
    "#     print (w)\n",
    "\n",
    "    for i in range(10):\n",
    "        sess.run(train_step, feed_dict={x: data, y_: label, lr: l})\n",
    "        \n",
    "#         if i % 1 == 0:\n",
    "        v, H, w = sess.run([loss, hess, parameters], feed_dict={x: data, y_: label, lr: l})    \n",
    "        nng = get_norm_grad()\n",
    "        eigs = sorted(np.linalg.eigvals(H)[0])\n",
    "        print(\"Epoch {}, accuracy {:.2f}%, loss {:.6f}, nng {:.4g}, nnw {:.4g}, high_eig {:.4g}, low_eig {:.4g}.\"\\\n",
    "                .format(i+1, get_accuracy()*100, v, nng, np.linalg.norm(w[:n_hidden]), max(eigs), min(eigs) ))\n",
    "        print (sess.run(grad[0], feed_dict={x: data, y_: label}))\n",
    "        print (w)\n",
    "#             h = sess.run([y], feed_dict={x: data, y_: label})    \n",
    "#             print (h)\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, accuracy 85.00%, loss 0.116878, nng 0.002133, nnw 1.361, high_eig 4.794, low_eig -8.59e-05.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108608, nng 0.001355, nnw 0.8117, high_eig 6.177, low_eig -9.647e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.789, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108401, nng 0.0004844, nnw 1.564, high_eig 4.442, low_eig 2.005e-06.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.304, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.116878, nng 0.00403, nnw 1.717, high_eig 3.526, low_eig -0.0002443.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.002, high_eig 0, low_eig 0.\n",
      "Epoch 160, accuracy 50.00%, loss 0.500000, nng 9.957e-05, nnw 0.2506, high_eig 0.1129, low_eig -2.465e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.7577, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.116879, nng 0.001866, nnw 1.697, high_eig 5.213, low_eig -0.0001647.\n",
      "Epoch 228, accuracy 86.00%, loss 0.118555, nng 9.816e-05, nnw 0.644, high_eig 5.862, low_eig -4.473e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.4325, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.697, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.3498, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108616, nng 0.0005173, nnw 1.198, high_eig 3.42, low_eig -0.0005653.\n",
      "Epoch 441, accuracy 86.00%, loss 0.118555, nng 9.953e-05, nnw 1.103, high_eig 4.943, low_eig -5.411e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.185, high_eig 0, low_eig 0.\n",
      "Epoch 192, accuracy 86.00%, loss 0.118555, nng 9.906e-05, nnw 0.5848, high_eig 6.587, low_eig 0.\n",
      "Epoch 307, accuracy 86.00%, loss 0.118555, nng 9.901e-05, nnw 1.15, high_eig 4.206, low_eig -3.569e-05.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108604, nng 0.001643, nnw 1.273, high_eig 6.874, low_eig -5.431e-05.\n",
      "Epoch 617, accuracy 86.00%, loss 0.118555, nng 9.999e-05, nnw 1.095, high_eig 6.123, low_eig -0.0001225.\n",
      "Epoch 1000, accuracy 86.00%, loss 0.118469, nng 0.001386, nnw 1.681, high_eig 5.162, low_eig -0.00465.\n",
      "Epoch 227, accuracy 50.00%, loss 0.500000, nng 9.927e-05, nnw 0.6788, high_eig 0.07804, low_eig -3.714e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.167, high_eig 0, low_eig 0.\n",
      "Epoch 291, accuracy 86.00%, loss 0.118555, nng 9.96e-05, nnw 0.8359, high_eig 4.77, low_eig -4.951e-05.\n",
      "Epoch 320, accuracy 86.00%, loss 0.118555, nng 9.978e-05, nnw 0.6835, high_eig 5.817, low_eig -3.262e-05.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108604, nng 0.0008071, nnw 1.229, high_eig 6.979, low_eig -0.0002067.\n",
      "Epoch 299, accuracy 86.00%, loss 0.118556, nng 9.971e-05, nnw 0.8904, high_eig 4.776, low_eig -1.205e-05.\n",
      "Epoch 350, accuracy 50.00%, loss 0.500000, nng 9.988e-05, nnw 2, high_eig 0.05118, low_eig -7.042e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.4945, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.9879, high_eig 0, low_eig 0.\n",
      "Epoch 222, accuracy 86.00%, loss 0.118555, nng 9.449e-05, nnw 0.5599, high_eig 7.752, low_eig -2.285e-05.\n",
      "Epoch 410, accuracy 86.00%, loss 0.118555, nng 9.97e-05, nnw 0.5623, high_eig 7.736, low_eig -9.306e-05.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.116625, nng 0.005483, nnw 1.937, high_eig 3.104, low_eig -0.0001103.\n",
      "Epoch 420, accuracy 50.00%, loss 0.500000, nng 9.971e-05, nnw 1.412, high_eig 0.03811, low_eig -9.197e-05.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108617, nng 0.001704, nnw 1.327, high_eig 5.682, low_eig -0.0002561.\n",
      "Epoch 266, accuracy 86.00%, loss 0.118555, nng 9.964e-05, nnw 0.8497, high_eig 4.595, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.1424, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.117929, nng 0.002186, nnw 0.798, high_eig 6.591, low_eig -0.002185.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.023, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.5179, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.9333, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 86.00%, loss 0.118375, nng 0.001092, nnw 1.234, high_eig 7.055, low_eig -0.0004299.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108610, nng 0.001462, nnw 1.85, high_eig 5.356, low_eig -4.215e-07.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.117883, nng 0.01119, nnw 2.42, high_eig 7.165, low_eig -0.0009373.\n",
      "Epoch 298, accuracy 86.00%, loss 0.118555, nng 9.855e-05, nnw 0.9089, high_eig 4.445, low_eig -1.564e-05.\n",
      "Epoch 162, accuracy 84.00%, loss 0.109045, nng 9.692e-05, nnw 1.326, high_eig 3.131, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.32, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.6946, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.109638, nng 0.001305, nnw 1.903, high_eig 4.919, low_eig -4.103e-05.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.109637, nng 0.0007009, nnw 2.254, high_eig 4.501, low_eig -1.166e-05.\n",
      "Epoch 391, accuracy 50.00%, loss 0.500000, nng 9.975e-05, nnw 1.696, high_eig 0.04699, low_eig -8.287e-05.\n",
      "Epoch 73, accuracy 50.00%, loss 0.250000, nng 1.613e-05, nnw 0.3439, high_eig 2, low_eig 0.\n",
      "Epoch 1000, accuracy 86.00%, loss 0.118244, nng 0.002273, nnw 1.389, high_eig 4.339, low_eig -0.003009.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108495, nng 0.0002588, nnw 1.724, high_eig 3.492, low_eig -1.436e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.87, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.116989, nng 0.001671, nnw 1.136, high_eig 7.348, low_eig -0.0001758.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.032, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 86.00%, loss 0.118556, nng 0.0001064, nnw 1.484, high_eig 4.766, low_eig -3.508e-05.\n",
      "Epoch 660, accuracy 86.00%, loss 0.118555, nng 5.583e-05, nnw 1.822, high_eig 4.563, low_eig -0.0005193.\n",
      "Epoch 597, accuracy 86.00%, loss 0.118555, nng 9.998e-05, nnw 1.209, high_eig 4.271, low_eig -0.0001603.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.116934, nng 0.0004319, nnw 1.717, high_eig 5.845, low_eig -1.796e-06.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.116627, nng 0.005309, nnw 1.808, high_eig 2.964, low_eig -0.0002223.\n",
      "Epoch 317, accuracy 50.00%, loss 0.500000, nng 9.926e-05, nnw 0.5717, high_eig 0.05592, low_eig -7.881e-05.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108752, nng 0.04675, nnw 0.6365, high_eig 7.505, low_eig -0.009378.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.109641, nng 0.006684, nnw 1.189, high_eig 6.794, low_eig -0.000243.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108799, nng 0.05534, nnw 0.7714, high_eig 7.733, low_eig -0.01039.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.312, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.369, high_eig 0, low_eig 0.\n",
      "Epoch 661, accuracy 86.00%, loss 0.118555, nng 9.975e-05, nnw 1.198, high_eig 4.232, low_eig -9.108e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.834, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.108791, nng 0.05484, nnw 1.163, high_eig 7.922, low_eig -0.01797.\n",
      "Epoch 768, accuracy 86.00%, loss 0.118555, nng 9.994e-05, nnw 1.051, high_eig 4.563, low_eig -0.0001535.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.23, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108608, nng 0.0007004, nnw 1.18, high_eig 3.27, low_eig -0.0002849.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.7317, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.117404, nng 0.005713, nnw 1.258, high_eig 8.01, low_eig -0.0005907.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.133, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108604, nng 0.0006375, nnw 1.36, high_eig 5.774, low_eig -6.93e-05.\n",
      "Epoch 1000, accuracy 86.00%, loss 0.118483, nng 0.001544, nnw 0.7878, high_eig 7.047, low_eig -0.01399.\n",
      "Epoch 218, accuracy 86.00%, loss 0.118555, nng 9.833e-05, nnw 0.5216, high_eig 7.738, low_eig -1.131e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.4281, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108777, nng 0.002015, nnw 0.9396, high_eig 4.642, low_eig -0.001152.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.117048, nng 0.002506, nnw 1.127, high_eig 4.444, low_eig -0.0001947.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.236, high_eig 0, low_eig 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.9238, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.8699, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 2.009, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 85.00%, loss 0.116932, nng 0.0004668, nnw 2.234, high_eig 5.935, low_eig -1.058e-06.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.116987, nng 0.001629, nnw 1.059, high_eig 7.497, low_eig -0.0001974.\n",
      "Epoch 419, accuracy 86.00%, loss 0.118555, nng 9.924e-05, nnw 1.238, high_eig 4.183, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.05828, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.315, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.6842, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.099, high_eig 0, low_eig 0.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.7303, high_eig 0, low_eig 0.\n",
      "Epoch 470, accuracy 86.00%, loss 0.118555, nng 9.9e-05, nnw 1.563, high_eig 4.55, low_eig -5.075e-05.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 1.895, high_eig 0, low_eig 0.\n",
      "Epoch 1000, accuracy 84.00%, loss 0.108605, nng 0.00135, nnw 0.7916, high_eig 5.787, low_eig -0.0007482.\n",
      "Epoch 1, accuracy 50.00%, loss 0.500000, nng 0, nnw 0.8461, high_eig 0, low_eig 0.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "l = 0.25\n",
    "dic = {}\n",
    "\n",
    "for _ in range(100):\n",
    "    \n",
    "    flag = 1\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    for i in range(1000):\n",
    "        sess.run(train_step, feed_dict={x: data, y_: label, lr: l})\n",
    "        nng = get_norm_grad()\n",
    "        if nng < 1e-4:\n",
    "            flag = 1\n",
    "            break\n",
    "\n",
    "    if flag == 1:\n",
    "        v, H, w = sess.run([loss, hess, parameters], feed_dict={x: data, y_: label, lr: l})    \n",
    "        nng = get_norm_grad()\n",
    "        eigs = sorted(np.linalg.eigvals(H)[0])\n",
    "        print(\"Epoch {}, accuracy {:.2f}%, loss {:.6f}, nng {:.4g}, nnw {:.4g}, high_eig {:.4g}, low_eig {:.4g}.\"\\\n",
    "                .format(i+1, get_accuracy()*100, v, nng, np.linalg.norm(w[:n_hidden]), max(eigs), min(eigs) ))\n",
    "#             h = sess.run([y], feed_dict={x: data, y_: label})    \n",
    "#             print (h)\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "3.19:\n",
    "\n",
    "Relu is helpful to obtain flat region. For example, zero is not a first order stationary point while using sigmoid, but \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
