{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11bcbb080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(3)\n",
    "\n",
    "def rand_cluster(n,c,r):\n",
    "    \"\"\"returns n random points in disk of radius r centered at c\"\"\"\n",
    "    x,y = c\n",
    "    points = []\n",
    "    for i in range(n):\n",
    "        theta = 2*math.pi*random.random()\n",
    "        s = r*random.random()\n",
    "        points.append((x+s*math.cos(theta), y+s*math.sin(theta)))\n",
    "    return points\n",
    "\n",
    "def rand_clusters(k,n,r, a,b,c,d):\n",
    "    \"\"\"return k clusters of n points each in random disks of radius r\n",
    "    where the centers of the disk are chosen randomly in [a,b]x[c,d]\"\"\"\n",
    "    clusters = []\n",
    "    for _ in range(k):\n",
    "        x = a + (b-a)*random.random()\n",
    "        y = c + (d-c)*random.random()\n",
    "        clusters.extend(rand_cluster(n,(x,y),r))\n",
    "    return clusters\n",
    "\n",
    "n = 50\n",
    "X = rand_clusters(2,50,1.8,-1,1,-1,1)\n",
    "data = np.array(X)\n",
    "label = np.transpose(np.array([[1]*n + [0]*n, [0]*n + [1]*n]))\n",
    "# label = np.array([1]*n + [0]*n)\n",
    "# print (data, label)\n",
    "\n",
    "plt.scatter(data[:n,0], data[:n,1], color=['red'])\n",
    "plt.scatter(data[n:,0], data[n:,1], color=['green'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape=shape, stddev=0.1)\n",
    "    return tf.get_variable(name=name, initializer=initial)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.get_variable(name=name, initializer=initial)\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2])\n",
    "y_ = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "#hidden layer\n",
    "# W_fc1 = weight_variable([2, 4], 'W1')\n",
    "# b_fc1 = bias_variable([4], 'b1')\n",
    "# h_fc1 = tf.sigmoid(tf.matmul(x, W_fc1) + b_fc1)\n",
    "# #output layer\n",
    "# W_fc2 = weight_variable([4, 2], 'W2')\n",
    "# b_fc2 = bias_variable([2], 'b2')\n",
    "# y = tf.sigmoid(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "\n",
    "n_input = 2\n",
    "n_hidden = 4\n",
    "n_hidden2 = 5\n",
    "n_output = 2\n",
    "# lmd = 1e-2\n",
    "lmd = 0\n",
    "# parameters = tf.Variable(tf.concat([tf.truncated_normal([n_input * n_hidden]), tf.zeros([n_hidden]),\\\n",
    "                            # tf.truncated_normal([n_hidden * n_output]), tf.zeros([n_output])],0))\n",
    "# parameters = tf.Variable(tf.concat([tf.truncated_normal([n_input * n_hidden]),\n",
    "#                                     tf.truncated_normal([n_hidden * n_output])\n",
    "#                                    ], 0))\n",
    "parameters = tf.Variable(tf.concat([tf.zeros([n_input * n_hidden]), tf.zeros([n_hidden]),\\\n",
    "                                    tf.zeros([n_hidden * n_output]), tf.zeros([n_output]),\\\n",
    "                                   ], 0))\n",
    "\n",
    "idx_from = 0 \n",
    "weights1 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_input*n_hidden]), [n_input, n_hidden])\n",
    "idx_from = idx_from + n_input*n_hidden\n",
    "biases1 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_hidden]), [n_hidden])\n",
    "hidden = tf.nn.relu(tf.matmul(x, weights1) + biases1)\n",
    "\n",
    "idx_from = idx_from + n_hidden\n",
    "weights2 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_hidden*n_output]), [n_hidden, n_output])\n",
    "idx_from = idx_from + n_hidden*n_output\n",
    "biases2 = tf.reshape(tf.slice(parameters, begin=[idx_from], size=[n_output]), [n_output])\n",
    "y = tf.nn.relu(tf.matmul(hidden, weights2) + biases2)\n",
    "\n",
    "\n",
    "weights = tf.concat([tf.reshape(weights1, [-1]), tf.reshape(weights2, [-1])], 0)\n",
    "regularizer = tf.nn.l2_loss(weights)\n",
    "\n",
    "los = tf.reduce_mean(tf.reduce_sum(tf.pow(y_ - y, 2), reduction_indices=[1])) #I also tried simply tf.nn.l2_loss(y_ - y)\n",
    "loss = los + lmd * regularizer\n",
    "\n",
    "\n",
    "grad = tf.gradients(loss, parameters)\n",
    "hess = tf.hessians(loss, parameters)\n",
    "\n",
    "lr = tf.placeholder(tf.float32, shape=[])\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "train_step = optimizer.apply_gradients(grads_and_vars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_accuracy():\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return sess.run(accuracy, feed_dict={x: data, y_: label})\n",
    "\n",
    "def get_norm_grad():\n",
    "    nng = 0.\n",
    "    for gv in grad:\n",
    "        # print(str(sess.run(gv[0], feed_dict={x: data, y_: label})) + \" - \" + gv[1].name)\n",
    "        g = sess.run(gv, feed_dict={x: data, y_: label})\n",
    "#         print (g)\n",
    "        nng += np.linalg.norm(g) ** 2\n",
    "    return np.sqrt(nng)\n",
    "\n",
    "def display(w):\n",
    "\n",
    "    idx_from = 0 \n",
    "    weights1 = np.reshape(w[idx_from: n_input*n_hidden], [n_input, n_hidden])\n",
    "    idx_from = idx_from + n_input*n_hidden\n",
    "    biases1 = np.reshape(w[idx_from: idx_from+n_hidden], [n_hidden])\n",
    "    idx_from = idx_from + n_hidden\n",
    "    weights2 = np.reshape(w[idx_from: idx_from+n_hidden*n_output], [n_hidden, n_output])\n",
    "    idx_from = idx_from + n_hidden*n_output\n",
    "    biases2 = np.reshape(w[idx_from: idx_from+n_output], [n_output])\n",
    "    print (weights1)\n",
    "    print (biases1)\n",
    "    print (weights2)\n",
    "    print (biases2)\n",
    "     \n",
    "def displayH(a):\n",
    "    a = np.array(a[0])\n",
    "#     print (\"Matrix[\"+(\"%d\" %a.shape[0])+\"][\"+(\"%d\" %a.shape[1])+\"]\")\n",
    "    rows = a.shape[0]\n",
    "    cols = a.shape[1]\n",
    "    for i in range(0, rows):\n",
    "        for j in range(0, cols):\n",
    "            print(\"%0.2g \" %a[i,j], end=\"\")\n",
    "        print ()\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, accuracy 75.00%, loss 0.328895, nng 0.0001646, nnw 31.2, high_eig 0.09032, low_eig 1.122e-05.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "dic = {}\n",
    "for _ in range(1):\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    flag = 0\n",
    "    for i in range(1000):\n",
    "        sess.run(train_step, feed_dict={x: data, y_: label, lr:1e-1})\n",
    "        nng = get_norm_grad()\n",
    "        if nng < 1e-4:\n",
    "            flag = 1\n",
    "            break\n",
    "\n",
    "    if flag != 3:\n",
    "#         if i % 40 == 0:\n",
    "        v, H, w = sess.run([loss, hess, parameters], feed_dict={x: data, y_: label})    \n",
    "        eigs = sorted(np.linalg.eigvals(H)[0])\n",
    "        print(\"Epoch {}, accuracy {:.2f}%, loss {:.6f}, nng {:.4g}, nnw {:.4g}, high_eig {:.4g}, low_eig {:.4g}.\"\\\n",
    "                    .format(i+1, get_accuracy()*100, v, nng, np.linalg.norm(w[:]), max(eigs), min(eigs) ))\n",
    "#         display(w)\n",
    "#         displayH(H)\n",
    "        dic[int(v * 1e5)] = dic.get(int(v * 1e5), []) + [(min(eigs), w)]\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 51, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 101, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 151, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 201, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 251, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 301, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 351, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 401, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 451, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 501, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 551, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 601, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 651, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 701, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 751, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 801, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 851, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 901, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 951, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1001, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1051, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1101, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1151, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1201, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1251, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1301, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1351, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1401, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1451, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1501, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1551, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1601, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1651, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1701, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1751, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1801, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1851, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1901, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n",
      "Epoch 1951, accuracy 50.00%, loss 1.000000, nng 0, nnw 0, high_eig 0, low_eig 0.\n",
      "[[ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]]\n",
      "[ 0.  0.  0.  0.]\n",
      "[[ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]\n",
      " [ 0.  0.]]\n",
      "[ 0.  0.]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "dic = {}\n",
    "\n",
    "for _ in range(1):\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "    flag = 0\n",
    "    for i in range(2000):\n",
    "        l = 10**(-(i+1)//1000)\n",
    "        sess.run(train_step, feed_dict={x: data, y_: label, lr: l})\n",
    "        nng = get_norm_grad()\n",
    "#         if nng < 4e-4:\n",
    "#             flag = 1\n",
    "#             break    \n",
    "            \n",
    "        if i % 50 == 0:\n",
    "            v, H, w = sess.run([loss, hess, parameters], feed_dict={x: data, y_: label})    \n",
    "            eigs = sorted(np.linalg.eigvals(H)[0])\n",
    "            print(\"Epoch {}, accuracy {:.2f}%, loss {:.6f}, nng {:.4g}, nnw {:.4g}, high_eig {:.4g}, low_eig {:.4g}.\"\\\n",
    "                        .format(i+1, get_accuracy()*100, v, nng, np.linalg.norm(w[:]), max(eigs), min(eigs) ))\n",
    "            display(w)\n",
    "#             displayH(H)\n",
    "            dic[int(v * 1e5)] = dic.get(int(v * 1e5), []) + [(min(eigs), w)]\n",
    "#             py = sess.run([y], feed_dict={x: data, y_: label})    \n",
    "#             print (py)\n",
    "#             break\n",
    "\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
